{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import zipfile\n",
    "from functools import partial\n",
    "\n",
    "manifest_json = \"\"\"\n",
    "{\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"manifest_version\": 2,\n",
    "    \"name\": \"Chrome Proxy\",\n",
    "    \"permissions\": [\n",
    "        \"proxy\",\n",
    "        \"tabs\",\n",
    "        \"unlimitedStorage\",\n",
    "        \"storage\",\n",
    "        \"<all_urls>\",\n",
    "        \"webRequest\",\n",
    "        \"webRequestBlocking\"\n",
    "    ],\n",
    "    \"background\": {\n",
    "        \"scripts\": [\"background.js\"]\n",
    "    },\n",
    "    \"minimum_chrome_version\":\"22.0.0\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "background_js = \"\"\"\n",
    "var config = {\n",
    "        mode: \"fixed_servers\",\n",
    "        rules: {\n",
    "          singleProxy: {\n",
    "            scheme: \"http\",\n",
    "            host: \"202.141.80.24\",\n",
    "            port: parseInt(3128)\n",
    "          },\n",
    "          bypassList: [\"foobar.com\"]\n",
    "        }\n",
    "      };\n",
    "chrome.proxy.settings.set({value: config, scope: \"regular\"}, function() {});\n",
    "function callbackFn(details) {\n",
    "    return {\n",
    "        authCredentials: {\n",
    "            username: \"gulat170123030\",\n",
    "            password: \"FTS6C3kq\"\n",
    "        }\n",
    "    };\n",
    "}\n",
    "chrome.webRequest.onAuthRequired.addListener(\n",
    "            callbackFn,\n",
    "            {urls: [\"<all_urls>\"]},\n",
    "            ['blocking']\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "#JUST RETURNS THE SOUPIFIED CONTENT OF THAT URL BY USING LXML PARSER\n",
    "def GET_SOUP(my_url):\n",
    "\turl = my_url\n",
    "\tresponse = requests.get(url)\n",
    "\thtml = response.content\n",
    "\tsoup = BeautifulSoup(html, features='lxml')\n",
    "\treturn soup\n",
    "\n",
    "#SCRAPE THE CONTENT OF THAT URL BY REPEATEDLY FINDING THE LOAD MORE BUTTON\n",
    "def scrapeData(url):\n",
    "    listOfComments=[]\n",
    "\n",
    "    driver_options = Options()\n",
    "    driver_options.add_argument(\"--no-sandbox\")\n",
    "    driver_options.add_argument(\"--start-maximized\")\n",
    "    driver_options.add_extension('proxy_auth_plugin.zip')\n",
    "\n",
    "    driver = webdriver.Chrome('/usr/bin/chromedriver', options=driver_options)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            loadMoreButton = driver.find_element_by_id(\"load-more-trigger\")\n",
    "            time.sleep(0.1)\n",
    "            loadMoreButton.click()\n",
    "            count = 0\n",
    "            time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            count += 1\n",
    "            if count == 20:\n",
    "                break\n",
    "\n",
    "    soup_3=BeautifulSoup(driver.page_source, features=\"lxml\")\n",
    "\n",
    "    for item in soup_3.findAll('div', attrs={'class': 'review-container'}):\n",
    "        try:\n",
    "            headComment = item.find('div', attrs={'class': 'lister-item-content'}).find('a').text.replace('\\n', '')\n",
    "        except Exception as e:\n",
    "            headComment = \"\"\n",
    "\n",
    "        try:\n",
    "            mainComment = item.find('div', attrs={'class': 'text show-more__control'}).text.replace('\\n', '')\n",
    "        except Exception as e:\n",
    "            mainComment = \"\"\n",
    "\n",
    "        try:\n",
    "            commentRating = item.find('span', attrs={'class': 'rating-other-user-rating'}).find('span').text\n",
    "        except Exception as e:\n",
    "            commentRating = \"\"\n",
    "\n",
    "        listOfComments.append([headComment, mainComment, commentRating])\n",
    "\n",
    "    driver.quit()\n",
    "    return [url, listOfComments]\n",
    "\n",
    "#GET THE FINAL URL USED FOR SCRAPPING THE DATA FROM THE MAIN PAGE BY GETTING INTO EACH MOVIE\n",
    "#THERE ARE SUCH HUNDRED MAIN PAGES CONTAINING HUNDRED MOVIES EACH\n",
    "def getURLs(url):\n",
    "    soup=GET_SOUP(url)\n",
    "    table = soup.find('div', attrs={'class': 'lister-list'})\n",
    "\n",
    "    listofMovieUrls=[]\n",
    "    for row in table.findAll('div', attrs={'class': 'lister-item mode-detail'}):\n",
    "        startTime = time.time()\n",
    "        print(\"Work under progress! Please do not interfere with the system\")\n",
    "\n",
    "        try:\n",
    "            rating = row.find('span', attrs={'class': 'ipl-rating-star__rating'}).text.replace('\\n', '')\n",
    "        except Exception as e:\n",
    "            rating = \"\"\n",
    "\n",
    "        try:\n",
    "            title = row.find('h3', attrs={'class': 'lister-item-header'}).find('a').text.replace('\\n', '')\n",
    "        except Exception as e:\n",
    "            title = \"\"\n",
    "\n",
    "        try:\n",
    "            url_2=\"https://www.imdb.com\"+row.find('h3', attrs={'class': 'lister-item-header'}).find('a').get(\"href\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        soup_2=GET_SOUP(url_2)\n",
    "\n",
    "        try:\n",
    "            url_3=\"https://www.imdb.com\" + soup_2.find('div', attrs={'class': 'user-comments'}).findAll('a')[4].get(\"href\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        listofMovieUrls.append({url_3: (title, rating)})\n",
    "        driver_options = Options()\n",
    "        driver_options.add_argument(\"--start-maximized\")\n",
    "        driver_options.add_extension('proxy_auth_plugin.zip')\n",
    "\n",
    "    print(\"page completed\")\n",
    "    return listofMovieUrls\n",
    "\n",
    "\n",
    "pluginfile = 'proxy_auth_plugin.zip'\n",
    "\n",
    "with zipfile.ZipFile(pluginfile, 'w') as zp:\n",
    "    zp.writestr(\"manifest.json\", manifest_json)\n",
    "    zp.writestr(\"background.js\", background_js)\n",
    "\n",
    "mainUrl = \"https://www.imdb.com/list/ls057823854/\"\n",
    "url = \"https://www.imdb.com/list/ls057823854/?sort=list_order,asc&st_dt=&mode=detail&page=\"\n",
    "\n",
    "FinalList=[]\n",
    "listOfUrls=[]\n",
    "listOfUrls.append(mainUrl)\n",
    "\n",
    "numPages = 100\n",
    "#GETTING THOSE HUNDRED MAIN PAGES\n",
    "for i in range(2, numPages+1):\n",
    "\tlistOfUrls.append(url+str(i))\n",
    "\n",
    "numCores = 4\n",
    "\n",
    "parProcess = Pool(numCores)\n",
    "result = parProcess.map(getURLs, listOfUrls)    # List of list of dicionaries\n",
    "# LIST OF LIST OF DICTIONARY WHERE [DICTIONARY CONTAINS (FINAL URL --> MOVIE NAME & RATING OF THE MOVIE ITSELF)], [ONE LIST IS THOSE 100 MOVIE ON A SINGLE PAGE]\n",
    "#[OUTER LIST IS BASICALLY FOR THOSE 100 MAIN PAGES]\n",
    "FinalList=[]\n",
    "finalDict = {}\n",
    "for i in result:\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            finalDict[k] = j[k]\n",
    "            FinalList.append(k)\n",
    "\n",
    "parProcesses = [Pool(numCores), Pool(numCores), Pool(numCores), Pool(numCores)]\n",
    "numMovies = len(FinalList)\n",
    "for i in range(0, 4):\n",
    "    # parProcess2 = Pool(numCores)\n",
    "    result2 = parProcesses[i].map(scrapeData, FinalList[(numMovies*i)//4:(numMovies*(i+1))//4])\n",
    "    # movie = [url, listOfComments]\n",
    "    for movie in result2:\n",
    "        movieName = finalDict[movie[0]][0]\n",
    "        movieRating = finalDict[movie[0]][1]\n",
    "        print(movieName)\n",
    "        outfile = open(\"./data/\"+movieName.replace(' ', '_').replace('\"', '_').replace('/', '_')+\"_\"+str(movieRating)+\".tsv\", \"w\")\n",
    "        writer = csv.writer(outfile, delimiter='\\t', lineterminator='\\n')\n",
    "        writer.writerow(['Comment Head','Comment Body', 'Comment Rating'])\n",
    "        writer.writerows(movie[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
